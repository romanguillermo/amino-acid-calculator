import requests
from bs4 import BeautifulSoup
import csv

# Function to scrape a page and return extracted data
def scrape_page(url):
    data_rows = []  # List to store extracted data rows
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        section_markers = soup.find_all('span', class_='tableSectionTitle', text='Essential Amino Acids')
        for marker in section_markers:
            data_container = marker.find_next_sibling('table')  # Adjust based on actual structure
            if data_container:
                rows = data_container.find_all('tr')
                for row in rows:
                    columns = row.find_all('td')
                    data_row = [col.text.strip() for col in columns]
                    if data_row:  # Make sure it's not an empty list
                        data_rows.append(data_row)
    else:
        print(f"Failed to fetch {url}")
    return data_rows

# Function to write data to a CSV file
def write_to_csv(data, filename='output.csv'):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['Column1', 'Column2', 'Column3'])  # Adjust column names based on your data
        writer.writerows(data)

# Starting URL or a list of URLs to start scraping from
start_urls = ['https://www.myfooddata.com']  # Example, adjust accordingly

all_data = []  # List to store data from all pages
for url in start_urls:
    page_data = scrape_page(url)
    all_data.extend(page_data)

# Write the aggregated data to CSV
write_to_csv(all_data, 'amino_acids_data.csv')
